# Report

First, let's start with what's done.
My strategy was to build something working end-to-end, then iterate.
I spent most of the time solving the data-loading problem.

The current flow looks like this:

For data loading, I use:
- [robolyst/streetview](https://github.com/robolyst/streetview) (for getting info about the nearest panorama and downloading panorama images)
- [sk-zk/streetlevel](https://github.com/sk-zk/streetlevel) (for getting metadata about panoramas and links to other panoramas)

Now, `GeoGuessrEnv` optionally receives `input_lat` and `input_lon` (done for debugging purposes) as well as geofence.
First, I use `streetview` to find the closest panorama and cache the mapping between the coordinates and the panorama ID. 
Next, I use `streetlevel` to retrieve metadata for that panorama, including links to adjacent ones. 
I then initiate a breadth-first search (BFS) to download and cache metadata for a fixed number of panoramas; that number should be configurable in the future.
Finally, I use `streetview` again to get the panorama images.

An important note is that both `streetview` and `streetlevel` do not use the Google Street View API to get data;
instead, they make calls to the Google Maps website, which could be blocked by Google if used excessively.

After the data is loaded in `reset`, we build a graph of panoramas that are connected and start an episode.

Currently, the env pulls data only from the Google street view, but [sk-zk/streetlevel](https://github.com/sk-zk/streetlevel?tab=readme-ov-file#functionality-overview) 
allows pulling data from other providers, like Bing Streetside or Yandex Panorama.


### Things to improve
- **Revisit the logic for placing links on the image**.  
  The relative positions of links from the metadata correspond to ground truth, but I'm not sure if I'm using the right heading to place each link. As an alternative, I could ignore directions from the metadata and compute the correct positions based on the panorama heading and coordinates.
- Revisit the click-handling logic.  
  Currently, it prioritizes absolute distance on the image, which is not correct for equirectangular images and does not align with the task description.
- Abstract the caching logic into a cache manager, with the ability to store data not only in the local filesystem
  but also in S3-like storage.
- Align with the task description for asset naming: currently, images saved to the local filesystem are named {pano_id}.jpg
- For the metadata data we currently use {pano_id}_mini.json which is used for building a graph.
- Right now, `OpenAIVisionAgent` was generated by Cursor and should be rewritten.  
  Specifically, it should keep a conversation history and use tool calls for structured output.
- Implement session reply functionality and offline mode support.
- Align with the task description for env parameters: make `provider`, `obs_size`, and `max_steps` configurable.
- For production use, consider reimplementing the data-loading logic to use an API key and also take
  `rate_limit_qps`, `max_fetch_retries`, and `min_capture_year` into account.


### Plan for the next steps
 - Add Python tests to verify working functionality, set up a better CI workflow.
 - Implement sampling according to the task description.
 - Implement session reply functionality and result writing.
 - Write a base agent and rewrite the OpenAI agent.
 - Make asset loading more robust; handle errors when an image is malformed, when there are no links in the image, etc.

> Note that the above list is not exhaustive. It would be great to 
> have a conversation with potential users of this environment to get feedback and guidance on 
> future improvements - particularly regarding offline mode logic, offline/online result writing, 
> required wrappers, link positioning, and similar aspects.
